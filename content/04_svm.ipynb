{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538fd765",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/chattopadhyayA/ml_pursue2025/blob/master/content/03_svm.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bba4a",
   "metadata": {},
   "source": [
    "As you can see from the previous example of decision trees. For classification, we are basically after a decision boundary. If we can somehow manage the decision boundary then we can safely classify the data in different categories. Towards that Support Vector Machines (SVM) are good tools.\n",
    "\n",
    "The core idea of SVM is to find a maximum marginal hyperplane (MMH) that best divides the dataset into two classes.\n",
    "\n",
    "```{figure} images/svm.png\n",
    "---\n",
    "height: 300px\n",
    "name:   svm\n",
    "align:  center\n",
    "---\n",
    "\"svm\"\n",
    "```\n",
    "\n",
    "**Support Vectors**: Support vectors are the datapoints closest to the hyperplane. These points are the most relevant to the construction of the classifier.\n",
    "\n",
    "**Hyperplane**: This is the decision boundary which seperates the two classes.\n",
    "\n",
    "\n",
    "**Margin**: This is the perpendicular distance from the hyperplane to the support vectors or closest points. The more the margin, the results are more better.\n",
    "\n",
    "\n",
    "## Working principle\n",
    "\n",
    "Without going much into the mathematics, the flowchart of SVM can be summarised as \n",
    "\n",
    "1. **Assume a Linear Separator:**: SVM tries to find a hyperplane $\\omega\\cdot x+b=0$, that separates the data into two classes.\n",
    "\n",
    "2. **Maximize the Margin**: Rather than checking all possible planes, SVM optimizes for the one that maximizes the margin â€” the distance between the hyperplane and the closest points from each class, called support vectors.\n",
    "\n",
    "\n",
    "$$\n",
    "Margin=\\frac{2}{||\\omega||}\n",
    "$$\n",
    "\n",
    "3. **Optimization Problem**: Minimise \n",
    "\n",
    "$$ \\frac{1}{2}||\\omega||^2 $$\n",
    "\n",
    "subject to $y_i(\\omega x_i+b)\\geq 1$, with $y_i$ being the label of the class. This is a quadratic programming problem.\n",
    "\n",
    "4.**Support Vectors Determine the Plane**: Only data points on the margin boundaries (i.e., support vectors) affect the position of the hyperplane.\n",
    "\n",
    "5. **Use Kernel Trick (if needed)**: For non-linear data, use a kernel function (e.g., Radial Basis Function (RBF), polynomial) to project data into higher dimensions where it becomes linearly separable. \n",
    "\n",
    "To illustrate the Kernel trick, consider that we have a two dimensional dataset about all the PURSUE interns, where we have the data amount of coffee consumed and whether or not they love cats. Let us represent by blue dots people who love cat and red dot to denote poeple who dont. \n",
    "\n",
    "\n",
    "![alt text](images/coffee_cat.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdea2f",
   "metadata": {},
   "source": [
    "**NOTE**: If time permits we will demonstrate SVM on the same dataset we used for the classification and later add the notebook here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe1090",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
